{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    聚类试图将数据集中的样本划分为若干个通常是不想交的子集，每个子集称为一个\"簇\"，通过这样的划分，每个簇可能对应一些潜在的概念（类别）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    K-均值算法是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类称不同的组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    K-均值是一个迭代算法，其基本思想是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. 首先选取k个随机的点，称为聚类中心。\n",
    "    2. 对于数据集中的每一个数据，按照距离K个聚类中心点的距离远近，将其与距离最近的中心点进行关联，与同一个中心点关联的所有点聚成一类。\n",
    "    3. 计算每个组的平均值，将该组所关联的中心点移动到平均值的位置上。\n",
    "    4. 重复以上步骤直至中心点不再变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定样本集D = {$x_1,x_2...,x_n$},所划分的聚类簇集C = {$C_1,C_2,...,C_k$},  \n",
    "那么k-均值最小化问题是要最小化所有的数据点与其所关联的聚类中心店之间的距离之和，因此k-均值的代价函数可以表示为:  \n",
    "J(x,C) = $\\sum_{i=1}^n||x_i - u_C||^2$  \n",
    "其中$u_C$代表与$x_i$最近的聚类中心点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾刚才给出的k-均值迭代算法，每次的循环都是用于减少聚类点所引起的误差，直至误差降到最低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-均值算法有一定的缺陷，在寻找聚类中心点的过程中，如果初始聚类中点选取的不合适可能偶尔会陷入局部最小值，即无法聚合。通常为了解决这个问题，我们可以多次运行k-均值算法，每次都重新进行随机初始化聚类中心，最后再比较多次运行的结果，选取代价函数最小的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，可以采用更好收敛的二分k-均值算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该算法首先将所有点作为一个簇，然后将该簇一分为二，然后选择其中一个簇继续划分，选择的依据是代价函数的值。不断迭代，直到得到指定的簇数目为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他的聚类方法包括：  \n",
    "    层次聚类：先将数据集中的每一个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类进行合并，该过程不断重复，直至达到预设的聚类簇个数。  AGNES\n",
    "    学习向量量化  LVQ\n",
    "    高斯混合聚类  \n",
    "    密度聚类  DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn中对应的模块是cluster\n",
    "```\n",
    "sklearn.cluster.KMeans(n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, tol=0.0001, precompute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm=’auto’)\n",
    "```\n",
    "参数包括：\n",
    "n_clusters 缺省值=8\n",
    "生成的聚类数，即产生的质心（centroids）数。  \n",
    "\n",
    "init：有三个可选值：'k-means++'， 'random'，或者传递一个ndarray向量。  \n",
    "此参数指定初始化方法，默认值为 ‘k-means++’。  \n",
    "‘k-means++’ 用一种特殊的方法选定初始质心从而能加速迭代过程的收敛  \n",
    "‘random’ 随机从训练数据中选取初始质心。  \n",
    "如果传递的是一个ndarray，则应该形如 (n_clusters, n_features) 并给出初始质心。  \n",
    "\n",
    "precompute_distances：三个可选值，‘auto’，True 或者 False。  \n",
    "预计算距离，计算速度更快但占用更多内存。  \n",
    "‘auto’：如果 样本数乘以聚类数大于 12million 的话则不预计算距离。This corresponds to about 100MB overhead per job using double precision.  \n",
    "True：总是预先计算距离。  \n",
    "False：永远不预先计算距离。  \n",
    " \n",
    "n_init 缺省值=10  \n",
    "用不同的质心初始化值运行算法的次数，最终解是在inertia意义下选出的最优结果。  \n",
    "\n",
    "max_iter 缺省值=300  \n",
    "执行一次k-means算法所进行的最大迭代数。\n",
    "\n",
    "tol 默认值= 1e-4\n",
    "与inertia结合来确定收敛条件。\n",
    "\n",
    "模型属性：\n",
    "cluster_centers_:每个簇中心的坐标\n",
    "labels_:每个样本对应的簇标签\n",
    "inertia_:每个簇组里的样本到簇中心的距离平方和。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
